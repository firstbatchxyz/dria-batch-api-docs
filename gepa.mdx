---
title: GEPA Prompt Evolution Service
---

## Overview

Dria's GEPA (Genetic-Pareto) Prompt Evolution Service automatically improves your prompts through iterative optimization. It accepts a seed prompt and evaluation dataset, then evolves better-performing prompts without manual tuning.

**Key benefits:**
- **Automatic prompt improvement** through reflective learning
- **Sample-efficient optimization** (up to 35x fewer iterations than traditional methods)
- **Transparent evolution history** with scores and ancestry tracking
- **No fine-tuning required** - works with existing models
- **High performance** - achieves 0.99+ scores on pedagogical tasks

**Ideal for:**
- Optimizing support response templates
- Refining instruction prompts
- Improving task-specific outputs
- Evolving domain-aware technical documentation
- Security code review templates

---

## Getting Started

### 1. Prepare Your Dataset

Create a JSON payload with:
- **Seed prompt**: Your initial prompt template
- **Dataset**: Examples with inputs and expected outputs (size must equal minibatchSize + paretoSize)
- **Evaluator**: Must use available models 
- **Budget**: Number of inference calls (minimum 10, recommended 25-50)
- **Model**: Must use available models 

Each dataset item needs:
- `input`: Object with variables matching your `datasetColumns`
- `expectedOutput`: The desired response (minimum 1 character)

**Important:** Dataset size must equal `minibatchSize + paretoSize`. The first `minibatchSize` items are used for training (feedback), and the next `paretoSize` items are used for validation (pareto). For example, if `minibatchSize: 3` and `paretoSize: 4`, you need exactly 7 dataset items in order.

### 2. Submit Execution

POST your payload to start evolution. The API returns an `evolutionId` immediately.

### 3. Monitor Progress

Poll the execution status every 10-30 seconds until completion. Status values:
- `pending`: Job queued
- `in_progress`: Actively evolving
- `completed`: Finished successfully
- `failed`: Encountered an error

### 4. Retrieve Results

Once completed, fetch the prompt evolution history to see how your prompt improved across generations.

---

## Python Example

```python
import requests
import time
import json

api_key = '<YOUR_API_KEY>'
base_url = 'https://mainnet.dkn.dria.co/api/v0'

# Step 1: Prepare payload
payload = {
    "customId": "my-prompt-evolution",
    "strategy": "RPM",
    "model": "gpt-4o",
    "datasetColumns": ["customer_name", "product", "issue"],
    "budget": 25,
    "minibatchSize": 2,
    "paretoSize": 2,
    "evaluator": json.dumps({"model": "gpt-4o-mini", "metric": "coherence"}),
    "dataset": [
        {
            "input": {"customer_name": "Alice", "product": "Router", "issue": "connection drops"},
            "expectedOutput": "Hi Alice, I've reset your router settings..."
        },
        {
            "input": {"customer_name": "Charlie", "product": "Router", "issue": "firmware update"},
            "expectedOutput": "Hi Charlie, I've updated your router firmware..."
        },
        {
            "input": {"customer_name": "Bob", "product": "Router", "issue": "slow speed"},
            "expectedOutput": "Hi Bob, I've optimized your QoS settings..."
        },
        {
            "input": {"customer_name": "Dana", "product": "Router", "issue": "WiFi range"},
            "expectedOutput": "Hi Dana, I've adjusted your antenna settings..."
        }
    ],
    "prompt": "Write a helpful support reply for {customer_name} about their {product}: {issue}"
}

# Step 2: Start execution
resp = requests.post(f'{base_url}/gepa/start_execution',
    headers={'x-api-key': api_key, 'Content-Type': 'application/json'},
    json=payload)
resp.raise_for_status()
evolution_id = resp.json()['evolutionId']
print(f'✅ Evolution started: {evolution_id}')

# Wait for initialization
time.sleep(5)

# Step 3: Monitor progress
while True:
    status_resp = requests.get(f'{base_url}/gepa/get_single_execution/{evolution_id}',
        headers={'x-api-key': api_key})
    status_resp.raise_for_status()
    data = status_resp.json()

    if 'prompt' in data:
        print(f"Status: {data['status']} | Gen: {data['prompt']['currentGeneration']} | Score: {data['prompt']['currentPromptScore']}")
    else:
        print(f"Status: {data['status']}")

    if data['status'] in ['completed', 'failed']:
        break
    time.sleep(10)

# Step 4: Get evolved prompts
if data['status'] == 'completed':
    prompts_resp = requests.get(f'{base_url}/gepa/get_execution_prompts',
        params={'evolutionId': evolution_id, 'page': 1, 'limit': 20},
        headers={'x-api-key': api_key})
    prompts_resp.raise_for_status()

    prompts = prompts_resp.json()['prompts']
    print(f'\n✅ Evolution complete! {len(prompts)} generations')
    for p in prompts:
        print(f"\nGen {p['generation']} (score: {p['score']}):\n{p['prompt'][:200]}...")
else:
    print('❌ Evolution failed')
```

---

## Example Payloads

### Example 1: Math Tutoring

```json
{
  "customId": "demo-arithmetic-001",
  "strategy": "RPM",
  "model": "gpt-4o",
  "datasetColumns": ["question", "hint"],
  "budget": 30,
  "minibatchSize": 3,
  "paretoSize": 4,
  "evaluator": "{\"model\": \"gpt-4o-mini\", \"metric\": \"exact_match\", \"threshold\": 0.95, \"partialCredit\": true}",
  "dataset": [
    {

      "input": {
        "question": "What is 17 + 26?",
        "hint": "Add tens first, then units."
      },
      "expectedOutput": "Answer: 43"
    },
    {

      "input": {
        "question": "Multiply 9 by 7.",
        "hint": "Use repeated addition if needed."
      },
      "expectedOutput": "Answer: 63"
    },
    {

      "input": {
        "question": "What is 125 - 67?",
        "hint": "Borrow carefully and explain."
      },
      "expectedOutput": "Answer: 58"
    },
    {

      "input": {
        "question": "What is 45 minus 18?",
        "hint": "Break 45 into 40 + 5."
      },
      "expectedOutput": "Answer: 27"
    },
    {

      "input": {
        "question": "Solve 12 * 14 using mental math.",
        "hint": "Split one factor into tens and ones."
      },
      "expectedOutput": "Answer: 168"
    },
    {

      "input": {
        "question": "Add 156 + 89.",
        "hint": "Round 89 to 90 first."
      },
      "expectedOutput": "Answer: 245"
    },
    {

      "input": {
        "question": "Divide 144 by 12.",
        "hint": "Think of 12 times table."
      },
      "expectedOutput": "Answer: 12"
    }
  ],
  "prompt": "You are an encouraging math tutor. Walk the student through each step, narrate your reasoning, and end with `Answer: <value>`."
}
```

### Example 2: Support Response

```json
{
  "customId": "support-aurora-router",
  "strategy": "RPM",
  "model": "gpt-4o",
  "datasetColumns": ["customer_name", "product", "issue_summary"],
  "budget": 25,
  "minibatchSize": 2,
  "paretoSize": 2,
  "evaluator": "{\"model\": \"gpt-4o-mini\", \"metric\": \"coherence\"}",
  "dataset": [
    {

      "input": {
        "customer_name": "Rory Chen",
        "product": "Aurora Mesh Router",
        "issue_summary": "intermittent drop-offs whenever video calls start"
      },
      "expectedOutput": "Hi Rory, I refreshed QoS and shared call-stability steps so video calls stay stable."
    },
    {

      "input": {
        "customer_name": "Priya Patel",
        "product": "Aurora Mesh Router",
        "issue_summary": "needs parental controls ready before weekend trip"
      },
      "expectedOutput": "Hi Priya, I set up device groups so you can enable parental controls in one tap before the trip."
    },
    {

      "input": {
        "customer_name": "Damian Wright",
        "product": "Aurora Mesh Router",
        "issue_summary": "wants one-sentence status summary"
      },
      "expectedOutput": "Damian, the mesh rollout finished a week early with redundant coverage verified."
    },
    {

      "input": {
        "customer_name": "Sam Lee",
        "product": "Aurora Mesh Router",
        "issue_summary": "roaming handoff assurance needed"
      },
      "expectedOutput": "Hi Sam, roaming profile is enabled and handoff latency is under 120 ms confirmed via live session trace."
    }
  ],
  "prompt": "Write a concise, empathetic support reply for {customer_name} about their {product}. Highlight the fix for: {issue_summary}. Close with an offer to help further."
}
```
---

## Real-World Evolution Example

Here's an actual GEPA execution showing how a prompt evolved from a simple instruction to a detailed, domain-aware template:

**Initial Prompt (Generation 0)** - Score: 0.38
```
Write a concise, empathetic support reply for {customer_name} about their {product}.
Highlight the fix for: {issue_summary}. Close with an offer to help further.
```

**Evolved Prompt (Generation 1)** - Score: 0.69 *(+82% improvement)*
```
Write a concise, empathetic support reply for {customer_name} regarding their {product}.
The response should address the specific accomplishment or reassurance related to their
issue, {issue_summary}, as follows:

1. For status summary concerns, acknowledge the completion of a recent project,
   emphasizing achievements such as the mesh rollout completion ahead of schedule
   and verification of redundant coverage. This reflects a successful network
   deployment, providing confidence in the network's reliability.

2. For roaming handoff assurance, confirm technical specifics such as the roaming
   profile being enabled and handoff latency being under 120 ms. Add credibility by
   mentioning a live session trace to validate these assertions, offering the customer
   reassurance about the seamless technology transition.

Close the email with a personalized offer to provide further assistance if needed,
maintaining a supportive and clear tone throughout the communication.

Include sender details like your name, position, and company name for professionalism
and context.
```

**Key Improvements Observed:**
- Structured approach with numbered sections
- Domain-specific knowledge extracted from dataset (QoS, mesh rollout, 120ms handoff latency)
- Concrete examples for different scenario types
- Professional formatting requirements
- Technical credibility elements (live session traces)

This demonstrates GEPA's reflective learning: the final prompt contains specific knowledge and patterns extracted from evaluating the dataset examples.

---

## API Reference

**Base URL:** `https://mainnet.dkn.dria.co/api/v0`

All endpoints require `x-api-key` header.

### `POST /gepa/start_execution`
Start a new prompt evolution. Returns `{"evolutionId": "uuid"}`.

**Required fields:**
- `customId`: Your identifier for this run
- `strategy`: `"RPM"` (Reflective Prompt Mutation) - currently the only available strategy
- `model`: Model name - must be one from the available models (e.g., `gpt-4o`, `gpt-4o-mini`)
- `datasetColumns`: Array of variable names
- `budget`: Inference budget (minimum 10, recommended 25-50)
- `minibatchSize`: Number of feedback examples per generation
- `paretoSize`: Number of pareto (validation) examples
- `evaluator`: JSON string with `model` field and metric config (e.g., `{"model": "gpt-4o-mini", "metric": "accuracy", "threshold": 0.9}`)
- `dataset`: Array of examples - size must equal `minibatchSize + paretoSize`. Each item needs `input` (object), `expectedOutput` (string, min length 1). First `minibatchSize` items are feedback, next `paretoSize` items are pareto
- `prompt`: Your seed prompt template

### `GET /gepa/get_all_executions`
List your executions. Supports `?page=1&limit=10` pagination.

### `GET /gepa/get_single_execution/{evolutionId}`
Get detailed status, current score, generation, and prompt.

### `GET /gepa/get_execution_prompts`
Get evolution history. Requires `?evolutionId=...`. Supports pagination.

---

## FAQ

**How long does evolution take?**
- Depends on budget and model. With `budget=30` and `gpt-4o`, expect 5-15 minutes.

**What's the minimum dataset size?**
- Dataset size must equal `minibatchSize + paretoSize`. Minimum recommended is 4 items (2 feedback + 2 pareto).

**Can I stop an execution?**
- Executions run until completion or budget exhaustion. Monitor via `get_single_execution`.

**What if my execution fails?**
- Common issues:
  - Dataset size doesn't match `minibatchSize + paretoSize`
  - Using models that are not supported
  - Missing `model` field in evaluator JSON
  - Invalid evaluator JSON format
  - Missing `input` or `expectedOutput` in dataset items
  - `expectedOutput` is empty string

**What models are supported?**
- `gpt-5-mini-2025-08-07` - Latest GPT-5 Mini
- `gpt-5-2025-08-07` - Latest GPT-5
- `gpt-4.1-2025-04-14` - GPT-4.1
- `gpt-4o-2024-08-06` - GPT-4o (recommended for best quality)
- `gpt-4o-mini-2024-07-18` - GPT-4o Mini (faster and cost-effective)

**What strategy should I use?**
- Currently only `RPM` (Reflective Prompt Mutation) is available. RPM works well for most use cases including pedagogical prompts, support responses, and code review templates.

**Do I need to provide a model in the evaluator?**
- Yes, the evaluator JSON must include a `model` field (e.g., `"model": "gpt-4o-mini"`)

---

## Supported Models

GEPA supports the following models:
- **gpt-5-mini-2025-08-07** - Latest GPT-5 Mini (fast and cost-effective)
- **gpt-5-2025-08-07** - Latest GPT-5 (highest quality)
- **gpt-4.1-2025-04-14** - GPT-4.1
- **gpt-4o-2024-08-06** - GPT-4o
- **gpt-4o-mini-2024-07-18** - GPT-4o Mini (balanced performance and cost)


---

## Additional Resources

- [GEPA Research Paper](https://arxiv.org/pdf/2507.19457) - Original technique by Agrawal et al.
- [Dria Batch Inference](https://dria.co/batch-inference) - Model availability and pricing
- **Support:** For questions or issues, mail inference@dria.co (include `customId` and `evolutionId` for faster assistance)

---

## Tutorials

Use the examples below to understand GEPA on three common workloads. Each tutorial shows real-world configurations that have been tested and successfully completed. You can copy these payloads and adapt them to your use case.

### Tutorial 1: Math Word Problems

**Goal:** Evolve a pedagogical prompt that walks students through mental arithmetic step-by-step.

**Strategy:** RPM (Reflective Prompt Mutation) with exact match evaluation

**Key Configuration:**
- `datasetColumns`: `["problem", "hint"]` - Two-column format for question and teaching hint
- `evaluator`: `{"model": "gpt-4o-mini", "metric": "exact_match", "threshold": 0.95, "partialCredit": true}` - Rewards both exact matches and near misses
- Dataset: 3 feedback items + 4 pareto items = 7 total (matching minibatchSize + paretoSize)
- Budget: 30 generations for thorough exploration

**Expected Results:** This configuration achieved a score of 0.993, evolving from a basic prompt to a detailed 40+ line guide with specific examples for addition, multiplication, and rounding strategies.

Example payload:

```json
{
  "customId": "math-tutor-run-001",
  "strategy": "RPM",
  "model": "gpt-4o",
  "datasetColumns": ["problem", "hint"],
  "budget": 30,
  "minibatchSize": 3,
  "paretoSize": 4,
  "evaluator": "{\"model\": \"gpt-4o-mini\", \"metric\": \"exact_match\", \"threshold\": 0.95, \"partialCredit\": true}",
  "dataset": [
    {

      "input": {
        "problem": "Add 47 and 38 without paper.",
        "hint": "Break numbers into tens and ones."
      },
      "expectedOutput": "Answer: 85"
    },
    {

      "input": {
        "problem": "What is 125 - 67?",
        "hint": "Borrow carefully and explain."
      },
      "expectedOutput": "Answer: 58"
    },
    {

      "input": {
        "problem": "Calculate 9 * 8.",
        "hint": "Use repeated addition or times table."
      },
      "expectedOutput": "Answer: 72"
    },
    {

      "input": {
        "problem": "Solve 12 * 14 using mental math.",
        "hint": "Split one factor into tens and ones."
      },
      "expectedOutput": "Answer: 168"
    },
    {

      "input": {
        "problem": "What is 200 - 87?",
        "hint": "Try counting up or borrowing."
      },
      "expectedOutput": "Answer: 113"
    },
    {

      "input": {
        "problem": "Add 156 + 89.",
        "hint": "Round 89 to 90 first."
      },
      "expectedOutput": "Answer: 245"
    },
    {

      "input": {
        "problem": "Divide 144 by 12.",
        "hint": "Think of 12 times table."
      },
      "expectedOutput": "Answer: 12"
    }
  ],
  "prompt": "You are an encouraging math tutor. Walk the student through each step, narrate your reasoning, and end with `Answer: <value>`."
}
```

**Workflow:**
1. Submit the payload using the Python example above
2. Monitor progress - expect 5-10 minutes for completion
3. Check score evolution through `get_single_execution` endpoint
4. Once completed, retrieve evolved prompts using `get_execution_prompts`
5. Compare initial vs final prompt to see improvements in pedagogical structure

**Real Results:** Initial prompt (score 0) → Final prompt (score 0.993) with detailed breakdowns like "Start with 47, add 30 to get 77, then add 8 to reach 85"

### Tutorial 2: Code Review with Security Focus

**Goal:** Evolve a prompt that reviews pull requests with focus on security vulnerabilities and best practices.

**Strategy:** RPM (Reflective Prompt Mutation) with exact match evaluation

**Key Configuration:**
- `datasetColumns`: `["diff", "toolFindings"]` - Code diff and static analysis output
- `evaluator`: `{"model": "gpt-4o-mini", "metric": "exact_match", "threshold": 0.8}` - Lower threshold for complex security analysis
- Dataset: 3 feedback items + 4 pareto items = 7 total (matching minibatchSize + paretoSize)
- Budget: 40 generations for comprehensive security pattern learning

**Expected Results:** This configuration achieved a score of 0.314 (starting from 0.14), evolving toward more structured security review methodology with specific examples for CORS policies, SQL injection, and path traversal.

Example payload:

```json
{
  "customId": "code-review-run-009",
  "strategy": "RPM",
  "model": "gpt-4o",
  "datasetColumns": ["diff", "toolFindings"],
  "budget": 40,
  "minibatchSize": 3,
  "paretoSize": 4,
  "evaluator": "{\"model\": \"gpt-4o-mini\", \"metric\": \"exact_match\", \"threshold\": 0.8}",
  "dataset": [
    {

      "input": {
        "diff": "@@ -24,6 +24,11 @@\n+  if (!user) {\n+    throw new Error('missing user');\n+  }\n+  if (!user.email) {\n+    return;\n+  }\n+  sendEmail(user.email);",
        "toolFindings": "WARN: sendEmail runs without rate limiting."
      },
      "expectedOutput": "1. Highlight missing rate limiting. 2. Suggest retry/backoff strategy."
    },
    {

      "input": {
        "diff": "@@ -88,7 +88,12 @@\n- const auth = req.headers['Authorization'];\n+ const auth = req.headers['authorization'];\n+ if (!auth) {\n+   res.status(401).send('missing token');\n+   return;\n+ }",
        "toolFindings": "WARN: continues execution after sending 401."
      },
      "expectedOutput": "Flag missing return, advise using early exit after response."
    },
    {

      "input": {
        "diff": "@@ -55,8 +55,15 @@\n+app.use(cors({origin: '*'}));",
        "toolFindings": "WARN: CORS policy allows all origins."
      },
      "expectedOutput": "Explain CORS risk with wildcard origin, recommend allowlist."
    },
    {

      "input": {
        "diff": "@@ -12,6 +12,16 @@\n  const query = `SELECT * FROM orders WHERE id = ${orderId}`;\n  return db.execute(query);",
        "toolFindings": "CRIT: SQL injection risk due to string interpolation."
      },
      "expectedOutput": "Explain SQL injection vector and recommend parameterized query."
    },
    {

      "input": {
        "diff": "@@ -45,9 +45,14 @@\n+  const token = jwt.sign({userId}, SECRET, {expiresIn: '365d'});",
        "toolFindings": "WARN: token expiration set to 1 year."
      },
      "expectedOutput": "Recommend shorter token lifetime (e.g., 15 minutes) with refresh mechanism."
    },
    {

      "input": {
        "diff": "@@ -78,6 +78,10 @@\n+  const filePath = path.join(UPLOAD_DIR, userInput);\n+  return fs.readFileSync(filePath);",
        "toolFindings": "CRIT: Path traversal vulnerability with unsanitized input."
      },
      "expectedOutput": "Identify path traversal risk, recommend path sanitization and validation."
    },
    {

      "input": {
        "diff": "@@ -92,7 +92,11 @@\n+  const result = eval(userFormula);",
        "toolFindings": "CRIT: eval() with user input enables code injection."
      },
      "expectedOutput": "Flag eval() as critical security issue, recommend safe expression parser."
    }
  ],
  "prompt": "You are a senior engineer reviewing pull requests. Summarize the key risks, reference static analysis findings explicitly, and propose concrete fixes."
}
```

**Workflow:**
1. Submit the payload with security-focused dataset
2. Monitor progress - expect 10-15 minutes for completion
3. Track improvements through generations
4. Once completed, compare initial vs evolved prompt for security pattern recognition

**Real Results:** Initial prompt (score 0.14) → Final prompt (score 0.314) with structured security review covering CORS validation, SQL injection detection, and path traversal prevention

---
